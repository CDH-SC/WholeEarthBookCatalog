{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Earth Book Catalog: SNAP\n",
    "\n",
    "During the 2019 Spring Semester, we used the Stanford Network Analysis Project Python Library to build and analyze our network <br>\n",
    "*For more information about snap: <br>\n",
    "http://snap.stanford.edu/snappy/index.html <br>\n",
    "For the documentation, tutorials, and reference manual:<br>\n",
    "http://snap.stanford.edu/snappy/doc/index.html<br>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> dataimport.py\n",
    "\n",
    "Creates the initial graph of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snap\n",
    "import time\n",
    "import os, sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *data import functions*\n",
    "\n",
    "**loadNodes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNodes(g, nodes, nodefile):\n",
    "  with open(nodefile,'rb') as nodefile:\n",
    "    fileReader = csv.reader(nodefile, delimiter='\\t')\n",
    "    for row in fileReader:\n",
    "      key = int(row[0])\n",
    "# if the node is not already in the dict + graph, add it\n",
    "      if nodes.get(key, -1) == -1:\n",
    "        value = row[1:]\n",
    "        if value != \"None\":\n",
    "            nodes.update({key:value})\n",
    "            g.AddNode(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loadRels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRels(g, nodes, relfile):\n",
    "  with open(relfile, 'rb') as relfile:\n",
    "    fileReader = csv.reader(relfile, delimiter='\\t')\n",
    "    for row in fileReader:\n",
    "        if nodes.get(int(row[0]))[0] != \"None\" and nodes.get(int(row[1]))[0] != \"None\":\n",
    "            g.AddEdge(int(row[0]), int(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**outputGraph:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputGraph(graph_name, g, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "    outputPath = os.path.join(output_dir, graph_name)\n",
    "    print (\"Saved in: %s\") % outputPath\n",
    "    FOut = snap.TFOut(outputPath)\n",
    "    g.Save(FOut)\n",
    "    FOut.Flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *main function*\n",
    "\n",
    "Uses the above functions to make graph with the small batch and prints the number of nodes and edges as well as iterating through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to create graph and print number of nodes and edges\n",
    "'''\n",
    "nodefile = \"data/small_batch_100000.tsv\"\n",
    "relfile = \"data/small_rel_batch_100000.tsv\"\n",
    "'''\n",
    "\n",
    "# uncomment to use smaller size\n",
    "nodefile = \"data/small_batch_100.tsv\"\n",
    "relfile = \"data/small_rel_batch_100.tsv\"\n",
    "\n",
    "\n",
    "nodes = { }\n",
    "g = snap.TNGraph.New()\n",
    "loadNodes(g, nodes, nodefile)\n",
    "loadRels(g, nodes, relfile)\n",
    "#outputGraph(\"dhc_graph1\", g, \"graph/\")\n",
    "# get number of nodes and edges\n",
    "print (\"Number of nodes: \") + str(g.GetNodes())\n",
    "print (\"Number of edges: \") + str(g.GetEdges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to iterate through all nodes\n",
    "print (\"Iterating through nodes\")\n",
    "for NI in g.Nodes():\n",
    "    print (\"%s: %s\") % (NI.GetId(), nodes.get(NI.GetId()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to iterate through edges\n",
    "print (\"Iterating through edges\")\n",
    "for EI in g.Edges():\n",
    "  print (\"from %s (%s) to %s (%s)\") % (EI.GetSrcNId(), nodes.get(int(EI.GetSrcNId()))[0], \n",
    "                                     EI.GetDstNId(), nodes.get(int(EI.GetDstNId()))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> stats.py\n",
    "\n",
    "Takes a graph and runs statistics on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.PrintInfo(g, \"Python type PNGraph\",\"stats.txt\", False)\n",
    "!cat \"stats.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> timeslice.py\n",
    "\n",
    "Takes in two years and creates two text files (nodes & edges) of all of the nodes and edges between those two years\n",
    "\n",
    "## *timeslice functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadNodes: loads all nodes into dictionary for access in const time in the next step\n",
    "def loadNodesToDict(nodes, nodefile):\n",
    "  with open(nodefile,'rb') as nodefile:\n",
    "    fileReader = csv.reader(nodefile, delimiter='\\t')\n",
    "    for row in fileReader:\n",
    "      key = int(row[0])\n",
    "# if the node is not already in the dict + graph, add it\n",
    "      if nodes.get(key, -1) == -1:\n",
    "        value = row[1:]\n",
    "        nodes.update({key:value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeslice: takes in two years, which will serve as a range of years, and two filenames, \n",
    "# nodeout and relout. All nodes that fall within the range of years will be printed to \n",
    "# nodeout, and all edges that have a node within the range will be printed to relout. \n",
    "def timeslice(lower, upper, nodeout, relout):\n",
    "    # editionIDs is a set because editions could be listed twice, and we want all unique editions\n",
    "    editionIDs = set()\n",
    "    for NI in g.Nodes():\n",
    "        # if the node is an edition do the following \n",
    "        if NI.GetId() >= 10000000 and NI.GetId() < 20000000:\n",
    "            # nodes.get(NI.GetId())[2] refers to the year of a node \n",
    "            curr = nodes.get(NI.GetId())[2]\n",
    "            # some year fields have multiple years in them, this if-elif block handles\n",
    "            # these cases separately by checking if the year(s) fall into the range,\n",
    "            # and adding them to the edition ID set if so\n",
    "            \n",
    "            # if the length of the year is four characters (field has only one year)\n",
    "            if len(curr) == 4:\n",
    "              # enclosed in try-catch for possibility that curr could be non-numeric\n",
    "                try:\n",
    "                    curr = int(curr)\n",
    "                    # if year falls into range, add to editionID set\n",
    "                    if curr >= lower and curr <= upper:\n",
    "                        editionIDs.add(NI.GetId())\n",
    "                except:\n",
    "                    pass\n",
    "            #  \"\\xe2\" is the '-', this means the year field contains two years \n",
    "            elif \"\\xe2\" in curr:\n",
    "                try:\n",
    "                    curr1 = int(curr[0:3])\n",
    "                    curr2 = int(curr[5:8])\n",
    "                    # if a year falls into range, add to editionID set\n",
    "                    if curr1 >= lower and curr1 <= upper:\n",
    "                        editionIDs.add(NI.GetId())\n",
    "                    if curr2 >= lower and curr2 <= upper:\n",
    "                        editionIDs.add(NI.GetId())\n",
    "                except:\n",
    "                    pass\n",
    "    relIDs = set()\n",
    "    file2 = open(relout, \"w\")\n",
    "    for EI in g.Edges():\n",
    "        # check both source and destination of the edge to see if they're part of \n",
    "        # the set constructed above, this will indicate whether the edition\n",
    "        # itself is from the year range\n",
    "        if EI.GetSrcNId() in editionIDs:\n",
    "            wr1 = str(EI.GetSrcNId()) + \"\\t\" + str(EI.GetDstNId()) + \"\\n\"\n",
    "            file2.write(wr1)\n",
    "            relIDs.add(EI.GetDstNId())\n",
    "        elif EI.GetDstNId() in editionIDs:\n",
    "            wr2 = str(EI.GetSrcNId()) + \"\\t\" + str(EI.GetDstNId()) + \"\\n\"\n",
    "            file2.write(wr2)\n",
    "            relIDs.add(EI.GetSrcNId())\n",
    "        else:\n",
    "            continue\n",
    "    file2.close()\n",
    "    \n",
    "    # take all the nodes that are in editionIDs or relIDs as a precaution (or the sets)\n",
    "    editionIDs = editionIDs | relIDs\n",
    "    # this is your final node output\n",
    "    file1 = open(nodeout, \"w\")\n",
    "    for ID in editionIDs:\n",
    "        file1.write(str(ID) + \"\\n\")\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *main function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of all nodes created \n",
    "nodes = { }\n",
    "loadNodesToDict(nodes, nodefile)\n",
    "nodeout = \"timeslice/nodeout.txt\"\n",
    "relout = \"timeslice/relout.txt\"\n",
    "# sample years\n",
    "timeslice(1943, 1945, nodeout, relout)\n",
    "\n",
    "print \"nodes from timeslice:\"\n",
    "!cat \"timeslice/nodeout.txt\"\n",
    "print \"edges from timeslice:\"\n",
    "!cat \"timeslice/relout.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> makegraph.py\n",
    "\n",
    "Makes graph of timeslice data\n",
    "\n",
    "## *makegraph functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different than original because file inputs are different, look at contents of nodeout.txt\n",
    "def loadNodes2(g, nodein):\n",
    "    nodefile = open(nodein, \"r\")\n",
    "    for row in nodefile:\n",
    "        g.AddNode(int(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRels2(g, relin):\n",
    "  with open(relin, 'rb') as relfile:\n",
    "    fileReader = csv.reader(relfile, delimiter='\\t')\n",
    "    for row in fileReader:\n",
    "        g.AddEdge(int(row[0]), int(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *main function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodein = \"timeslice/nodeout.txt\"\n",
    "relin = \"timeslice/relout.txt\"\n",
    "g = snap.TNGraph.New()\n",
    "loadNodes2(g, nodein)\n",
    "loadRels2(g, relin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> community.py\n",
    "\n",
    "Takes a graph and returns community info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "g = snap.ConvertGraph(snap.PUNGraph, g)\n",
    "\n",
    "# conduct community analysis on a subgraph created previously \n",
    "CmtyV = snap.TCnComV()\n",
    "modularity = snap.CommunityCNM(g, CmtyV)\n",
    "cnt = 0\n",
    "size = 0\n",
    "commSize = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the output, specifically including the size of the community and each node's type (for future filtering)\n",
    "for Cmty in CmtyV:\n",
    "    print \"Community: \" + str(cnt)\n",
    "    size = 0\n",
    "    for NI in Cmty:\n",
    "            nodeType = \"None\"\n",
    "            if NI >= 0 and NI < 10000000:\n",
    "                nodeType = \"Place\"\n",
    "            elif NI >= 10000000 and NI < 20000000:\n",
    "                nodeType = \"Edition\"\n",
    "            elif NI >= 20000000 and NI < 30000000:\n",
    "                nodeType = \"Publisher\"\n",
    "            elif NI >= 30000000 and NI < 40000000:\n",
    "                nodeType = \"Person\"\n",
    "            print str(NI) + \": \" + nodes.get(NI)[0] + \" (\" + nodeType + \")\"\n",
    "            size += 1\n",
    "    commSize[cnt] = size\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankfile = \"community/ranks.txt\"\n",
    "# sort communities from largest to smallest and print to rank file\n",
    "sort = sorted(commSize, key=commSize.get, reverse=True)\n",
    "f = open(rankfile, \"w\")\n",
    "f.write(\"Ranking by Community Size\\n\")\n",
    "for num in range(0, len(commSize)):\n",
    "    f.write(\"%s: %s\\n\" % (sort[num], commSize[sort[num]]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat \"community/ranks.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> analysis.py\n",
    "\n",
    "Takes a list of IDs and returns files of all people, publishers, and places in a community with those IDs\n",
    "\n",
    "## analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relatedNodes(comfile, rankfile, ids, prefix):\n",
    "    communities = []\n",
    "    places = [] \n",
    "    pubs = []\n",
    "    people = []\n",
    "    ranks = []\n",
    "    f = open(comfile).read().splitlines()\n",
    "    curr = 0\n",
    "    for line in f:\n",
    "        if line.split(': ')[0] == \"Community\":\n",
    "            curr = int(line.split(': ')[1])\n",
    "            continue\n",
    "        elif int(line.split(\": \")[0]) in ids:\n",
    "            communities.append(curr)\n",
    "\n",
    "    comFlag = False\n",
    "    for line in f:\n",
    "        if line.split(\": \")[0] == \"Community\":\n",
    "            if int(line.split(\": \")[1]) in communities:\n",
    "                comFlag = True\n",
    "                curr = int(line.split(': ')[1])\n",
    "            else:\n",
    "                comFlag = False\n",
    "        elif comFlag: \n",
    "            # categorizes all lines with a node into preson, place, or publisher\n",
    "            # and adds to the appropriate list\n",
    "            split = line.split(\"(\")\n",
    "            lineType = split[len(split)-1]\n",
    "            if lineType == \"Person)\":\n",
    "                elem = (line, curr)\n",
    "                people.append(elem)\n",
    "            elif lineType == \"Place)\":\n",
    "                elem = (line, curr)\n",
    "                places.append(elem)\n",
    "            elif lineType == \"Publisher)\":\n",
    "                elem = (line, curr)\n",
    "                pubs.append(elem)\n",
    "    # print rank ordered list of communities that the IDs in question are part of \n",
    "    rankfile = open(rankfile).read().splitlines()\n",
    "    for line in rankfile:\n",
    "        currCom = int(line.split(\": \")[0])\n",
    "        currRank = int(line.split(\": \")[1])\n",
    "        if currCom in communities:\n",
    "            rank = (currCom, currRank)\n",
    "            ranks.append(rank)\n",
    "    printOutput(communities, people, places, pubs, ranks, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing (0, 1, or 2 specifies a type)\n",
    "# 0 for printing people/places/publishers\n",
    "# 1 for removing duplicates in printing\n",
    "# 2 for the printing of ranks \n",
    "def printHelp(listo, filename, splitter, opt):\n",
    "    if opt == 0:\n",
    "        f = open(filename, \"w\")\n",
    "        for p in listo:\n",
    "            wr = str(p[1]) + \"   ***   \" + str(p[0].split(splitter)[0]) + \"\\n\"\n",
    "            f.write(wr)\n",
    "        f.close()\n",
    "    elif opt == 1:\n",
    "        f = open(filename, \"w\")\n",
    "        pSet = set(listo)\n",
    "        for p in pSet:\n",
    "            wr = str(p[0].split(splitter)[0]) + \"\\n\"\n",
    "            f.write(wr)\n",
    "        f.close()\n",
    "    else:\n",
    "        f = open(filename, \"w\")\n",
    "        for p in listo:\n",
    "            wr = \"Community \" + str(p[1]) + \" rank: \" + str(p[0]) + \"\\n\"\n",
    "            f.write(wr)\n",
    "        f.close()\n",
    "    print(\"Printed to \" + filename + \", size of \" + str(len(listo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printOutput(communities, people, places, pubs, ranks, prefix):\n",
    "    printHelp(people, prefix+\"people.txt\", \"(Person)\", 0)\n",
    "    printHelp(places, prefix+\"places.txt\", \"(Place)\", 0)\n",
    "    printHelp(pubs, prefix+\"pubs.txt\", \"(Publisher)\", 0)\n",
    "    #printHelp(people, prefix+\"upeople.txt\", \"(Person)\", 1)\n",
    "    #printHelp(places, prefix+\"uplaces.txt\", \"(Place)\", 1)\n",
    "    #printHelp(pubs, prefix+\"upubs.txt\", \"(Publisher)\", 1)\n",
    "    printHelp(ranks, prefix+\"ranks.txt\", \" \", 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *main function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = sys.argv[1]\n",
    "path = \"test/\" + name\n",
    "os.mkdir(path, 0755)\n",
    "listo = []\n",
    "for i in range(2, len(sys.argv)):\n",
    "    listo.append(int(i))\n",
    "\n",
    "relatedNodes(\"community.txt\", \"community/ranks.txt\", listo, \"test/\" + name + \"/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
